{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Every computer program, at it's most elementary form, can be boiled down to a series of basic arithmetic operations and functions. From the definition of the derivative, we know that we can __always__ calculate it by applying the chain rule repeatedly across the function. Therefore, we can find the derivative of every single function by applying automatic differentiation.\n",
    "\n",
    "Automatic differentiation is necessary, because other forms of differentiation (namely, symbolic and numerical) have difficulties with converting a computer program into a single full mathematical expression, which automatic differentiation does not struggle with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background\n",
    "\n",
    "The whole point of automatic differentiation is to break down complex problems into digestable chunks, but there are a few mathematical concepts you will need to be familiar with to understand our implementation.\n",
    "\n",
    "### <u>Chain Rule:</u>\n",
    "The Chain Rule is essential when calcluating derivative and is taught in introductory calculus classes around the world. The chain rule is as follows:\n",
    "$$\\frac{dy}{dx}=\\frac{dy}{du}*\\frac{du}{dx}$$\n",
    "Essentially, when you take the derivative of a function, you must take into consideration the derivatives of its inputs. Giving an example with real numbers:\n",
    "$$y=e^{x^2}$$\n",
    "$$\\frac{dy}{dx}=e^{x^2}2x$$\n",
    "\n",
    "### <u>Graph Structure of Calculations:</u>\n",
    "Using a graph structure is a helpful way to visualize a complicated function and the steps that are required to evaluate it. Constructing a graph is simple, and it will be helpful to look at an example:\n",
    "\n",
    "\n",
    "![](graph.png)\n",
    "\n",
    "We start by initalizing the first nodes to our inputs. As we trace across edges between nodes, we evaluate the given functions using the one or more nodes that are the origin(s) of a specific edge. As we make our way across the graph, we will complete all steps needed to evaluate the complicated function and eventually be left with its overall output in the graph's final node.\n",
    "\n",
    "\n",
    "### <u>Elementary Functions:</u>\n",
    "1. A background in functions that are common in mathematics is also necessary to understand this project. Of course, the four elementary functions $+, -, *,$ and $/$ will be present throughout.\n",
    "2. Trigonometric functions are also important to know and understand, these being $sin(x)$, $cos(x)$, and $tan(x)$. Additionally, inverse trig functions and hyperbolic functions can be used.\n",
    "3. Raising a number to a power is another operations we will need in this project. That includes raising a number to negative powers or a power between $0$ and $1$. Note that variables can exist in either the base (rational powers of $x$) or in the exponent (exponential functions).\n",
    "4. Finally, logaritms might be present in our project. Logrithms have a base and an input. If $b$ is your base, $n$ is your input, and $x$ is your output then:\n",
    "$$x = log_b n$$\n",
    "$$b^x = n$$\n",
    "\n",
    "### <u>Constants:</u>\n",
    "The constant $e$, Euler's number, might come up as well. $e\\approx 2.72$ and appears in many functions. For example, $e$ is the base of a natural-log.\n",
    "\n",
    "The constant $\\pi$ is also very important. $\\pi \\approx 3.14$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use\n",
    "\n",
    "### How to Install\n",
    "Our package will be available for download using the command `python3 -m pip install -i https://test.pypi.org/simple/ cs107-team38-ad`.\n",
    "\n",
    "Users will also need to download numpy using the command `python3 -m pip install -i numpy`. When we first implemented our package, we thought that we needed to automate this installation. However, we were told in office hours on 12/9 that the inclusion of requirements.txt was sufficient.\n",
    "\n",
    "### How to operate\n",
    "After a user has installed our package, package can be accessed through importing autodiff_package. This can be done through `import autodiff_package as ad` or similarly. Users have access to a number of functions:\n",
    "\n",
    "1. `grad()` is vanilla forward mode automatic differentiation.\n",
    "    - This function takes as input either a singleton or a vector of lambda functions, one or multiple values to evaluate the functions at, and an optional seed vector\n",
    "    - It returns the output for each function (in an array) evaluated at the input values, and:\n",
    "        - either a two dimensional array of arrays (containing the partial derivatives) as the jacobian,\n",
    "        - or an array of gradients for each function, if there is a seed input.\n",
    "2. `reverse()` is reverse mode automatic differentiation\n",
    "    - This function takes as input the same as grad(), and outputs the same as well. The difference between the two is the under the hood implementation.\n",
    "3. Various mathematical functions from functions.py, like `log`, trig functions, `exp`, etc. These functions must be used to create the lambda functions for input in `grad()` or `reverse()`, (instead of using similar libraries like numpy), in order for the automatic differentiation to work.\n",
    "\n",
    "A quick demo below calls our forward on a few different functions, including:\n",
    "\n",
    "$f(x) = 3x^2 + 2.5x +2$, evaluated at $x = 2$\n",
    "\n",
    "$g(x,y,z) = \\begin{pmatrix} \\frac{1}{2}x^2 - \\text{log}_2(y) + z \\\\ x \\cdot y + z    \\end{pmatrix}$, evaluated at $x = 6$, $y = 4$, $z = -3$\n",
    "\n",
    "We then re-evaluate the vector function g, with the seed $\\begin{bmatrix} 1 \\\\ 0 \\\\ 0 \\end{bmatrix}$. \n",
    "\n",
    "Finally, we also evaluate the function $h(x) = \\text{sin}(x) + 2x$ using reverse mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grad demo:\n",
      "\n",
      "\n",
      "The function f(x) = 3x^2 + 2.5x + 2 evaluated at 2 equals: [19.0]\n",
      "The derivative of that function evaluated at 2 equals: [14.5]\n",
      "\n",
      "\n",
      "The functions f(x,y,z) = (1/2)x^2 - log_2(y) + z, f(x,y,z) = x/z + y evaluated at x=6,y=4,z=-3 equals: [13.0, 21]\n",
      "The partial derivatives of those functions, at each variable, equals: [[6.0, -0.36067376022224085, 1], [4, 6, 1]]\n",
      "\n",
      "\n",
      "The second function, input with the seed [1,2,3], returns for the jacobian: [6.0, 4]\n",
      "\n",
      "Reverse demo:\n",
      "The function f(x) = sin(x) + 2x evaluated with REVERSE MODE at 2 equals: [4.909297426825682]\n",
      "The derivative of that function evaluated at 2 equals: 1.5838531634528576\n"
     ]
    }
   ],
   "source": [
    "# These first two lines are not needed by a user who has downloaded our function\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "# User's file starts here\n",
    "import autodiff_package as ad\n",
    "\n",
    "# Inputs from the user\n",
    "val_singleton = 2\n",
    "fun_singleton = lambda x: 3.0 * x * x + 2.5 * x + 2.0\n",
    "\n",
    "val_multiple = [6,4,-3]\n",
    "fun_multiple_1 = lambda x: 1/2 * x[0] ** 2 - ad.logbase(x[1],2) + x[2]\n",
    "fun_multiple_2 = lambda x: x[0] * x[1] + x[2]\n",
    "fun_vector = [fun_multiple_1, fun_multiple_2]\n",
    "\n",
    "\n",
    "# grad call\n",
    "answer1,jacobian1 = ad.grad(fun_singleton, val_singleton)\n",
    "answer2,jacobian2 = ad.grad(fun_vector, val_multiple)\n",
    "\n",
    "# Printing answers\n",
    "print('Grad demo:')\n",
    "print(\"\\n\")\n",
    "print(f'The function f(x) = 3x^2 + 2.5x + 2 evaluated at {val_singleton} equals: {answer1}')\n",
    "print(f'The derivative of that function evaluated at {val_singleton} equals: {jacobian1}')\n",
    "print(\"\\n\")\n",
    "print(f'The functions f(x,y,z) = (1/2)x^2 - log_2(y) + z, f(x,y,z) = x/z + y evaluated at x=6,y=4,z=-3 equals: {answer2}')\n",
    "print(f'The partial derivatives of those functions, at each variable, equals: {jacobian2}')\n",
    "\n",
    "\n",
    "# Seed input\n",
    "seed = [1,0,0]\n",
    "\n",
    "_,jacobian3 = ad.grad(fun_vector,val_multiple,seed)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f'The second function, input with the seed [1,2,3], returns for the jacobian: {jacobian3}')\n",
    "\n",
    "# Reverse mode demo:\n",
    "# Inputs from the user\n",
    "val = 2\n",
    "fun = lambda x: ad.sin(x)+2*x\n",
    "\n",
    "# grad call\n",
    "answer, jacobian = ad.reverse(fun, val)\n",
    "\n",
    "# Printing answers\n",
    "print('\\nReverse demo:')\n",
    "print(f'The function f(x) = sin(x) + 2x evaluated with REVERSE MODE at {val} equals: {answer}')\n",
    "print(f'The derivative of that function evaluated at {val} equals: {jacobian[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "This is our current software organization:\n",
    "```\n",
    "team38\n",
    "├── LICENSE\n",
    "├── README.md\n",
    "├── pyproject.toml\n",
    "├── .github/workflows\n",
    "│   ├── codeCoverage.yml\n",
    "│   └── tests.yml\n",
    "├── docs\n",
    "│   ├── milestone1.md\n",
    "│   ├── milestone2_progress.md\n",
    "│   ├── milestone2.ipynb\n",
    "│   └── documentation.ipynb\n",
    "├── src/autodiff_package\n",
    "│   ├── __init__.py\n",
    "│   ├── __main__.py\n",
    "│   ├── differentiate.py\n",
    "│   ├── functions.py\n",
    "│   └── node.py\n",
    "└── tests\n",
    "    ├── run_tests.sh\n",
    "    ├── test_differentiate.py\n",
    "    ├── test_dualnums.py\n",
    "    └── test_node.py\n",
    "```\n",
    "\n",
    "The src/autodiff_package directory is where the actual package logic will takes place. This includes differentiate.py which `grad` and `reverse` and is the general control center, functions.py which includes non dunder functions that can be used in inputted functions, and node.py that defines our node and dualnum functionality. We originally had dualnums.py as well, but after implementing reverse mode we realized the Node class did all of what the DualNumber class did and more. Forward mode now uses Node instead of DualNumber without issue.\n",
    "\n",
    "We implemented \\_\\_init\\_\\_.py to allow users to easily access functions without many layers in import statements. For example, users can do `import autodiff_package as ad` followed by `ad.grad()` instead of `from autodiff_package import differentiate` followed by `differentiate.grad()`. This makes our package much more usable. Additionally, the \\_\\_init\\_\\_.py file lets python know that this is, in fact, a package. \n",
    "\n",
    "Additionally, we have a tests directory which 1:1 mirrors the package, which allows us to take advantage of python's unit testing. These tests are run similar to HW4, using continuous integration, GitHub Actions, and our yml files.\n",
    "\n",
    "A guide on how a user can install our package was a requirement for the Software Organization section, but this was previously covered in How to Use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation Details\n",
    "\n",
    "### Core Data Structures\n",
    "\n",
    "Our entire project implementation centers around the Node data structure. A Node is a small data structure evolved from a dual number that has attributes of self.real, self.dual, self.child, and self.id, and is defined in the Node class of node.py. It has the capacity to run both forward and reverse mode with it's different attributes, which will be described in a subsequent subsection. \n",
    "\n",
    "### Core Classes\n",
    "\n",
    "Our core class is Node. We only have one core class because this data structure can handle both forward and reverse mode automatic differentiation. There is also a test class. \n",
    "\n",
    "### Important Attributes\n",
    "\n",
    "#### Attributes of the Node class:\n",
    "\n",
    "self.real: Stores the value of the node as is being evaluated by a function. For input nodes, the self.real stores the initial value, and in the root node, self.real stores the final evaluated function at the inputted values. This attribute is used in both forward and reverse mode, for example in forward pass to evaluate the function. \n",
    "\n",
    "self.dual: self.dual exists alongside self.real, and is useful only for forward mode automatic differentiation. As we evaluate the function by sequentially performing operations on the self.real value, we simultaneously perform the derivative of the operation on the self.dual value, initially set to 1 for input nodes, to keep track of the derivate throughout the function. In the final node of forward mode differentiation, while self.real has the evaluated value, self.dual contains that variable's partial derivative. \n",
    "\n",
    "self.id: This is only useful for reverse mode automatic differentiation. It is used to store the value of the partial derivative of the function in terms of the node based on the leaf node it references. This is updated whenever a node is reached when traversing down the tree in the reverse pass. Therefore when the reverse function is done traversing down the tree, the self.id of the nodes of the input values is equal to the partial derivative of the function in terms of that node, otherwise known as the adjoint. This can then be used by the reverse mode function to produce the list of partial derivative values for each function inputted and return the Jacobian.\n",
    "\n",
    "self.child: Self.child is also only useful for reverse mode automatic differentiation. This is where nodes keep track of the children that underwent some operation to lead to them. For example, in the Extension example below, C.child contains information about nodes A and node B, namely a reference to each node and the operation that transformed them into node C. The key aspect is that the transformation depends on the operation, and can also be interpreted as the derivative of that operation at the given input values. This allows the creation of our linked list, which allows reverse passes back from the root node to all of the leaves to calculate the Jacobian and, if desired, the directional derivative. \n",
    "\n",
    "### External Dependencies\n",
    "Our package requires numpy to function, and our test suit requires pytest. These two packages can be install with simple pip commands.\n",
    "\n",
    "### Elementary Functions\n",
    "\n",
    "There are a number of elementary functions defined in functions.py available to users. These include trigonometric functions, inverse trig, hyperbolic, exponential, log (with variable base), logistic, and square root. Additionally dunder methods have been implemented in the Node class to handle simpler functions like addition, powers, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extension\n",
    "\n",
    "### Milestone 2 Future Feature Section\n",
    "\n",
    "First and foremost, we expect one of our extensions to be the addition of reverse mode. We have learned about this option in class and feel confident that we will be able to implement it ourselves. This feels like the most natural addition to our project and will allow our project to cover more bases in terms of efficency.\n",
    "\n",
    "Reverse mode is different from forward mode as it traverses a computation graph once and then computes the gradient in a traversal in the opposite direction. It is much more efficient when many variables are present. We have bookmarked the node.py file to be used in our implementation. Reverse mode seems to be a more complicated software problem to solve, so our main issue will most likely be working through solutions and ironing out bugs -- we are all confident that we are up to the task.\n",
    "\n",
    "If we finish this implementation in enough time, we are tentatively interested in also working on higher-order derivatives. However, this is entirely predicated on us finishing reverse mode, as we would prefer to have a thorough implementation of a common extension rather than trying to spread ourselves too thin.\n",
    "\n",
    "### M2 Feedback\n",
    "We did not receive any feedback specifically on our plan for the future feature, but in looking back at what we had planned, we were able to implement every part. We created a node.py file that was an evolution from our former Dual Number implementation because it built on the real and dual attributes to contain an id function for keeping track of which node was which as well as a child attribute to keep track of the paths between nodes. It was definitely a more complicated software problem to solve, as we predicted, but we were up to the task and created a successful implementation for any function whose operations are comprised of our defined methods and helper functions. \n",
    "\n",
    "### Current Implementation of Extension\n",
    "\n",
    "For our project extension, we chose to implement reverse mode automatic differentiation. Reverse mode is a variation of forward mode automatic differentiation that lends itself particularly well to evaluating the Jacobian of functions with large numbers of input variables. This is because, rather than having to make a pass for each partial derivative like forward mode, reverse mode can calculate the entire Jacobian matrix with one forward pass and one reverse pass at a given input. Then, calculating the directional derivative is as simple as finding the dot product between the $m$ corresponding components of the Jacobian matrix with a seed vector. Below, we will review a high level overview of the theory behind reverse mode differentiation, as well as a more in depth explanation of our implementation.\n",
    "\n",
    "<img src=\"reverse_viz.jpeg\">\n",
    "\n",
    "Reverse mode differentiation begins with a forward pass, calculating the intermediate nodes as the function evaluates until it reaches the final, root node. However, unlike forward mode, we must retain references to each node's children and the operations that brought each node to its parent. This is because, in the reverse pass, we will follow these paths back down from the root to any specific leaf to calculate the partial derivative with respect to that leaf. Specifically, we want to keep track of the derivative of the operation that took any child to its parent. In the image above, we can see this stored in the child attribute. For example, for node C, we keep track of the fact that to get from A to C, we multiplied A by B.real, and for B to C, we multiplied B by A.real. The path is also the derivative for the operation, which in this case is multiplication. The derivative of dC/dA is B.real, and the derivative of dC/dB is A.real. In this way, for any given operation, we retain information relating a parent node to its children. This information is crucial for completing the reverse pass, shown below for both leaves A and B. \n",
    "\n",
    "<img src=\"partial_a.jpeg\">\n",
    "<img src=\"partial_b.jpeg\">\n",
    "\n",
    "As we recursively traverse back down this graph, represented in our code effectively as a linked list, we use the stored information about each child's path to calculate partial derivatives. This is done by multiplying paths in sequence and adding paths in parallel. In this small example, A becomes C by multiplying by B.real, which is 3. Then, the C node paths to the D node through division, which, with these values, evaluates to multiplying by -1/18. In parallel, A is used directly to calculate D as well, and undergoes a change by a factor of 1/6. We multiply -1/18 by 3, and add it to 6, to find that the partial derivative with respect to A is 0. We repeat this process w.r.t. B, and find the partial derivative is -1/9. \n",
    "\n",
    "<img src=\"directional.jpeg\">\n",
    "\n",
    "One of the greatest benefits of reverse mode implementation is the ease at which one can find the directional derivative at a certain seed vector. Because reverse mode calculates every partial derivative in one pass, finding directional derivatives with functions with many inputs becomes computationally much quicker. With the outputted Jacobian, we only need to take the dot product with the seed vector to find the magnitude of our function at in that seed's direction. Here, with seed vector of $p$ = [ 0 , 1 ], we find the magnitude of the directional derivative is -1/9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Broader Impact and Inclusivity Statement\n",
    "\n",
    "### Broader Impacts and Implcations\n",
    "\n",
    "Our package and automatic differentiation in general have large implications across several relevant fields.\n",
    "\n",
    "A large field that takes advantage of automatic differnentiation is machine learning, where frameworks like Flux.jl, Pytorch, and Tensorflow take advantage of these algorithms to solve complex equations efficiently. Machine learning is a powerful tool that must be used ethically and with broader impacts in mind; theoretically, a nefarious party could use our software to aid in an unethical use of ML. Some ethical issues in ML include privacy, bias, and transparency.\n",
    "\n",
    "On the other had, ML is currently being used to solve some of the world's must challenging and pressing issues, so our software could just as well be put to good, worthwhile use. For example, the recent rise of ChatGPT shows how far AI has come and opens the door for us to wonder how far the technology will progress in the future. It is important for us to recognize the dichotomy of negative and positive impacts of this project and others we develop.\n",
    "\n",
    "Another important field that automatic differentiation can be applied to is the medical field. An example of this is intensity modulated radiation therapy (IMRT). This is a procedure used to treat patients with cancer or other tumors. IMRT requires optimization using a very precise Jacobian, as can be provided with automatic differentiation, in order to accurately assess the priority that different tissues should be treated with. In this case, an accurate Jacobian can be lifesaving.\n",
    "\n",
    "Additionally, automatic differentiation can be used in weather modeling. In the case of the MM5 Weather Model, automatic differentiation is used to make a model to make sensitivity predictions and determine the accuracy of the atmospheric predictions. This could likewise be applied to other climate modeling. Therefore, our automatic differentiation project could be used to ensure that weather models are able to provide the most accurate and sensitive predictions as possible, allowing for people to adequately prepare for weather changes, including potentially life threatening natural disasters.\n",
    "\n",
    "There are many automatic differentiation packages that users can choose from, and users of ours or any others should carefully consider the ethical implications of their own projects as we do here.\n",
    "\n",
    "### Software Inclusivity\n",
    "\n",
    "We believe that software inclusivity is of the utmost importance and highly value diversity, equity, and the freedom of use of our project. Because of this, we chose to use the MIT Licence for this package. This allows for users to obtain, use, and modify the software without charge, ensuring that there are no financial barriers to use and that are users are able to be from diverse economic backgrounds. Additionally, the permissions given allowing modification of the software allows from users from diverse backgrounds of all types to contribute their ideas and improvements to the software.\n",
    "\n",
    "Despite our best attempts to make the software as inclusive as possible, this software, as all software does, still has some limitations to access that we were unable to prevent at this time. Due to the respect we have for diversity and our users, we acknowledge these limitations here and our attempts to lessen them. For one, the software and its documentation is written in English, which could serve as a barrier to international users and those less familiar with the English language. It makes use of Python, which is primarily in English.\n",
    "\n",
    "Additionally, the software for automatic differentiation, which although the basic principles were explained previously in the document, assumes a basic understanding of derivatives and calculus. This means that a user without sufficient mathematical knowledge may not be able to use the software to its full abilities.\n",
    "\n",
    "Finally, our value for inclusivity is not limited to the users of this software but also applies to us as we created it. We ensured that all members of the team were encouraged and able to contribute to the project. We worked as a unified team, contributing each of our perspectives and ideas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future\n",
    "\n",
    "There are a few things that could be added to this package.\n",
    "\n",
    "First, we could add more functions to functions.py. We have a great base of functions that can be utilized by users to calculate many derivatives, but we could add more and make our suite more robust.\n",
    "\n",
    "Second, we'd like to implement an extension pertaining to a field that we are interested. Half of our group are pre-med students, so applying our technology to the field of medicine would be challenging but rewarding. In medicine, automatic differentiation is used in many pertinent topics, including COVID-19. A quick google search lead us to [this](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7306401/) study. Connecting our work to a study such as this or taking on our own research project is something that interests us greatly.\n",
    "\n",
    "Third, another area that interests a majority of group members in statistics and probability. Preforming more complicated mathematics within our package could allow us to venture into ML through papers such as [this](https://proceedings.neurips.cc/paper/2020/file/7a674153c63cff1ad7f0e261c369ab2c-Paper.pdf) and apply our finding to optimizing strategy in games such as poker or chess.\n",
    "\n",
    "Finally, implementing other proposed extensions such as optimizing our functions, calculating higher-order derivatives, generating graphs is always a possiblitity and would make our package better overall."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Oct 30 2022, 18:10:46) \n[Clang 12.0.0 (clang-1200.0.32.2)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c45d2d2507d6553f9850b8ddc5c07fa65f562af9b09fc6e3d5771bbbdf7b3f0f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
